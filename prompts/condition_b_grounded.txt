SYSTEM:
You are a tutoring assistant helping a student study optimization methods for machine learning. 
Answer the student's question accurately using your knowledge.

You MUST output your response in the following JSON format:
{
  "thinking_process": "Your step-by-step reasoning",
  "sources_used": "What knowledge sources you relied on",
  "answer": "Your response to the student",
  "confidence": "high/medium/low"
}

IMPORTANT: Use Socratic method. Guide the student toward understanding rather than just giving answers. Ask clarifying questions where appropriate.

The questions will be from this knowledge set (attached latex). Confirm you understand the requirements.

```latex
\documentclass[12pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{fancyhdr}

% Page formatting to simulate academic density
\geometry{margin=1in}
\onehalfspacing

% Header settings
\pagestyle{fancy}
\fancyhf{}
\rhead{Optimization Methods in Machine Learning}
\lhead{Academic Draft}
\cfoot{\thepage}

% Theorem environments
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\title{\textbf{Optimization Methods in Machine Learning:\\ From Gradient Descent to Adaptive Moment Estimation}}
\author{Your Name \\ Department of Computer Science \\ University of LaTeX}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Optimization lies at the heart of modern machine learning, acting as the engine that translates data into model intelligence. This paper provides a comprehensive analysis of first-order optimization algorithms used to minimize empirical risk. We begin by establishing the mathematical foundations of convexity and smoothness. We then explore the evolution of gradient-based methods, moving from standard Gradient Descent (GD) to Stochastic Gradient Descent (SGD), and finally to advanced adaptive methods such as Adam. Special attention is paid to the convergence properties of these algorithms, specifically analyzing the impact of the Lipschitz constant and strong convexity parameters on the error bounds. We demonstrate how momentum terms and adaptive learning rates can overcome the limitations of static update rules in high-dimensional non-convex landscapes.
\end{abstract}

\newpage

\section{Introduction}

Machine learning problems are frequently cast as optimization problems where the goal is to find a set of parameters that minimize a scalar-valued objective function. Whether training a simple linear regression model or a deep neural network with billions of parameters, the fundamental mechanism involves iteratively adjusting weights to reduce prediction error.

In this document, we explore the landscape of continuous optimization. We focus primarily on first-order methods, which utilize gradient information to guide the search for an optimal solution. As we traverse the hierarchy of algorithms, we will explicitly define the notation and variables required to understand the mechanics of learning.

The paper is structured as follows: Section 2 establishes the mathematical preliminaries, defining the dataset, the loss function, and critical properties of the objective surface such as Lipschitz continuity. Section 3 introduces the workhorse of machine learning: Gradient Descent and its stochastic variants. Section 4 analyzes how historical gradient information can be leveraged via momentum and adaptive learning rates to accelerate convergence. Section 5 provides a rigorous convergence analysis, linking the properties defined in Section 2 to the algorithms defined in Sections 3 and 4. Finally, Section 6 concludes with a discussion on the gap between optimization and generalization.

\newpage

\section{Mathematical Preliminaries}

Before deriving specific update rules, we must rigorously define the problem setting and the variables governing the optimization landscape.

\subsection{Problem Formulation}

We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$, consisting of $N$ pairs of inputs and targets. Specifically, we let $x_i$ represent the input feature vector for the $i$-th sample, and $y_i$ represent the corresponding label.

The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$, where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.

Let $\ell(x_i, y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:

\begin{equation} \label{eq:empirical_risk}
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(x_i, y_i; \theta)
\end{equation}

To perform optimization, we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently, $-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.

\subsection{Regularization}

To prevent overfitting, it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient, a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:

\begin{equation} \label{eq:regularized_loss}
\mathcal{J}(\theta) = \mathcal{L}(\theta) + \frac{\lambda}{2} \|\theta\|^2
\end{equation}

Throughout the remainder of this paper, unless specified otherwise, we will focus on minimizing $\mathcal{L}(\theta)$, assuming $\lambda=0$ for simplicity, though the derivations easily extend to the regularized case.

\subsection{Curvature and Smoothness Definitions}

The speed at which an optimizer converges depends heavily on the geometry of the loss function. We must define two critical constants that describe this geometry.

First, we define the Lipschitz constant. Let $L > 0$ be the Lipschitz constant for the gradient of the loss function. This parameter quantifies the smoothness of the function; effectively, it bounds how fast the gradient can change. A function $\mathcal{L}$ has $L$-Lipschitz continuous gradients if for all $\theta_1, \theta_2 \in \mathbb{R}^d$:

\begin{equation} \label{eq:lipschitz}
\| \nabla \mathcal{L}(\theta_1) - \nabla \mathcal{L}(\theta_2) \| \le L \| \theta_1 - \theta_2 \|
\end{equation}

Second, we define the strong convexity parameter. Let $\mu > 0$ denote the strong convexity constant. This parameter provides a lower bound on the curvature of the function. If $\mathcal{L}$ is twice differentiable, this implies the smallest eigenvalue of the Hessian is at least $\mu$. Formally, a function is $\mu$-strongly convex if for all $\theta_1, \theta_2$:

\begin{equation} \label{eq:strong_convexity}
\mathcal{L}(\theta_2) \ge \mathcal{L}(\theta_1) + \nabla \mathcal{L}(\theta_1)^T (\theta_2 - \theta_1) + \frac{\mu}{2} \| \theta_2 - \theta_1 \|^2
\end{equation}

As we will see in Section 6, the ratio between $L$ and $\mu$ (the condition number) dictates the theoretical convergence speed of gradient descent.

\newpage

\section{Gradient Descent Methods}

With the objective function $\mathcal{L}(\theta)$ and gradient $\nabla \mathcal{L}$ defined in the previous section, we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.

\subsection{Batch Gradient Descent}

The most fundamental optimization algorithm is Gradient Descent. We introduce the iteration counter $t$, representing the discrete time step of the algorithm. We also define the learning rate $\eta$ (sometimes denoted as $\alpha$), which is a positive scalar determining the step size.

In Batch Gradient Descent, we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:

\begin{equation} \label{eq:gd_update}
\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
\end{equation}

While theoretically sound, calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$, which is computationally prohibitive for large datasets.

\subsection{Stochastic Gradient Descent (SGD)}

To address the computational bottleneck, we approximate the full gradient using a single sample or a subset of samples. Let $B$ denote the batch size. In standard SGD, $B=1$, while in Mini-batch SGD, $1 < B < N$.

Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1, \dots, N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:

\begin{equation} \label{eq:stochastic_grad}
g_t = \nabla_\theta \left( \frac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i, y_i; \theta_t) \right)
\end{equation}

The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:

\begin{equation} \label{eq:sgd_update}
\theta_{t+1} = \theta_t - \eta g_t
\end{equation}

Ideally, the stochastic gradient is an unbiased estimator of the true gradient, meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However, the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5, this noise prevents exact convergence unless the learning rate $\eta$ decays over time.

\subsection{Relationship to Regularization}

If we return to the regularized loss $\mathcal{J}(\theta)$ defined in Equation (\ref{eq:regularized_loss}), which included the parameter $\lambda$, the gradient update changes slightly. The gradient of the regularization term $\frac{\lambda}{2}\|\theta\|^2$ is simply $\lambda \theta$. Thus, the update rule incorporating weight decay is:

\begin{equation} \label{eq:weight_decay_update}
\theta_{t+1} = \theta_t - \eta \left( \nabla \mathcal{L}(\theta_t) + \lambda \theta_t \right) = (1 - \eta \lambda)\theta_t - \eta \nabla \mathcal{L}(\theta_t)
\end{equation}

This formulation explicitly shows how $\lambda$ acts to decay the weights at every step $t$, keeping the parameters bounded.

\newpage

\section{Momentum and Adaptive Methods}

Standard SGD often struggles in areas where the surface curves much more steeply in one dimension than in another (common in local optima). To mitigate this, we introduce the concept of "momentum," which accumulates a velocity vector from past gradients.

\subsection{SGD with Momentum}

We define a new variable, the momentum vector $v_t$, which stores a moving average of past gradients. We also introduce a momentum coefficient $\beta \in [0, 1)$, which controls how much history is retained.

Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3, the momentum update involves two steps. First, we update the velocity:

\begin{equation} \label{eq:momentum_velocity}
v_{t+1} = \beta v_t + \eta g_t
\end{equation}

Note that some formulations scale the gradient by $(1-\beta)$ rather than $\eta$. Using the formulation above, the parameter update is:

\begin{equation} \label{eq:momentum_update}
\theta_{t+1} = \theta_t - v_{t+1}
\end{equation}

This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine), the terms in $v_t$ cancel out. If the direction is consistent, $v_t$ accelerates.

\subsection{Adaptive Moment Estimation (Adam)}

While momentum adapts the step direction, it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.

Adam maintains two history variables. We recall $v_t$ as the first moment estimate (mean of gradients), and we define a new variable $s_t$ as the second moment estimate (uncentered variance of gradients). We utilize two decay rates: $\beta_1$ for the first moment and $\beta_2$ for the second moment.

At iteration $t$, we compute the raw moments based on the gradient $g_t$:

\begin{align} \label{eq:adam_raw}
v_t &= \beta_1 v_{t-1} + (1 - \beta_1) g_t \nonumber \\
s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2
\end{align}

Here, $g_t^2$ indicates the element-wise square of the gradient vector. Because $v_t$ and $s_t$ are initialized to 0, they are biased toward zero during the initial time steps. We compute bias-corrected estimates $\hat{v}_t$ and $\hat{s}_t$:

\begin{equation} \label{eq:adam_bias_correction}
\hat{v}_t = \frac{v_t}{1 - \beta_1^t}, \quad \hat{s}_t = \frac{s_t}{1 - \beta_2^t}
\end{equation}

Finally, we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:

\begin{equation} \label{eq:adam_update}
\theta_{t+1} = \theta_t - \eta \frac{\hat{v}_t}{\sqrt{\hat{s}_t} + \epsilon}
\end{equation}

This equation demonstrates the power of adaptive methods: parameters with large gradients (large $s_t$) have their effective learning rate reduced, while parameters with sparse, small gradients are scaled up.

\newpage

\section{Convergence Analysis}

In this section, we analyze the theoretical convergence of Gradient Descent. We rely heavily on the Lipschitz constant $L$ and the strong convexity parameter $\mu$ defined in Section 2.

\subsection{Setup and Assumptions}

Let $\theta^*$ denote the global minimizer of the loss function $\mathcal{L}(\theta)$. Our goal is to bound the squared distance error $\|\theta_t - \theta^*\|^2$ as $t \rightarrow \infty$.

We assume the standard Batch Gradient Descent update rule from Equation (\ref{eq:gd_update}): $\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)$.

\subsection{Convex Convergence (Lipschitz Smoothness)}

First, we analyze the case where $\mathcal{L}$ is convex and $L$-smooth, but not necessarily strongly convex (i.e., $\mu = 0$).

We begin by expanding the squared norm of the error at step $t+1$:

\begin{align} \label{eq:error_expansion}
\| \theta_{t+1} - \theta^* \|^2 &= \| \theta_t - \eta \nabla \mathcal{L}(\theta_t) - \theta^* \|^2 \nonumber \\
&= \| \theta_t - \theta^* \|^2 - 2\eta \nabla \mathcal{L}(\theta_t)^T (\theta_t - \theta^*) + \eta^2 \| \nabla \mathcal{L}(\theta_t) \|^2
\end{align}

Using the properties of convexity and the Lipschitz smoothness condition (Eq \ref{eq:lipschitz}), one can derive that for a learning rate $\eta \le 1/L$, the objective values converge at a rate of $O(1/t)$. This is known as sublinear convergence.

\subsection{Linear Convergence (Strong Convexity)}

Now, consider the case where the function is $\mu$-strongly convex ($\mu > 0$). Recall from Equation (\ref{eq:strong_convexity}) that this implies a quadratic lower bound on the growth of the function.

We can strengthen the bound on the error contraction. Substituting the strong convexity inequalities into the expansion in Equation (\ref{eq:error_expansion}), we can derive a contraction factor. Specifically, if we set the learning rate $\eta = \frac{1}{L}$ (or more optimally dependent on the condition number), the relationship between successive errors is:

\begin{equation} \label{eq:linear_contraction}
\| \theta_{t+1} - \theta^* \|^2 \le \left( 1 - \frac{\mu}{L} \right) \| \theta_t - \theta^* \|^2
\end{equation}

Let $\kappa = L/\mu$ be the condition number of the Hessian. The term $(1 - \mu/L)$ can be rewritten as $(1 - 1/\kappa)$. Since $\mu \le L$, this factor is strictly less than 1. Applying this inequality recursively over $t$ iterations yields:

\begin{equation} \label{eq:convergence_rate}
\| \theta_t - \theta^* \|^2 \le \left( 1 - \frac{\mu}{L} \right)^t \| \theta_0 - \theta^* \|^2
\end{equation}

This result represents **linear convergence** (in optimization terminology, confusingly, this refers to exponential decay of the error). It highlights the critical relationship between the geometry of the loss function and the optimization speed.

\subsection{Implications for Adaptive Methods}

Recall the Adam update in Equation (\ref{eq:adam_update}). While Adam generally converges faster empirically on non-convex problems (like neural networks), proving its convergence in the convex setting is subtle. In fact, for certain definitions of $\beta_1, \beta_2$, Adam can fail to converge even on convex problems.

However, if the effective step size (scaled by $1/\sqrt{\hat{s}_t}$) is bounded appropriately, adaptive methods can be shown to enjoy convergence rates similar to SGD, specifically $O(1/\sqrt{t})$ for general convex functions, adapting to the local geometry defined by $L$.

\newpage

\section{Conclusion}

This paper has reviewed the mathematical foundations and algorithmic developments in optimization for machine learning. We started by defining the empirical risk $\mathcal{L}(\theta)$ over a dataset $\mathcal{D}$ and explored the geometry of this landscape through the Lipschitz constant $L$ and strong convexity parameter $\mu$.

We detailed the progression of algorithms from the fundamental Batch Gradient Descent to Stochastic Gradient Descent. By introducing the learning rate $\eta$ and the momentum vector $v_t$, we showed how modern optimizers navigate complex loss landscapes. The derivations of Adam further illustrated how tracking the second moment $s_t$ allows for parameter-specific adaptation.

Our convergence analysis in Section 5 linked these variables together, demonstrating that the convergence rate depends critically on the condition number $L/\mu$. While theoretical guarantees are strongest for convex problems, these insights drive the design of heuristics used in deep learning.

Future work in this field focuses on closing the gap between the optimization of the training loss and the generalization to unseen data. While we can drive $\nabla \mathcal{L} \to 0$ efficiently, ensuring that $\theta^*$ yields low error on test distributions remains the ultimate challenge of learning theory.

\newpage

\begin{thebibliography}{9}

\bibitem{bishop}
Bishop, C. M. (2006).
\textit{Pattern Recognition and Machine Learning}.
Springer.

\bibitem{goodfellow}
Goodfellow, I., Bengio, Y., \& Courville, A. (2016).
\textit{Deep Learning}.
MIT Press.

\bibitem{kingma}
Kingma, D. P., \& Ba, J. (2014).
Adam: A Method for Stochastic Optimization.
\textit{arXiv preprint arXiv:1412.6980}.

\bibitem{nesterov}
Nesterov, Y. (1983).
A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$.
\textit{Doklady AN SSSR}, 269(3), 543-547.

\end{thebibliography}

\end{document}
```

=== EXTRACTED VARIABLES ===
~id	~labels	latex_sample	created	name	source	paragraph_samples	pos	type	latex	hash	abs_pos
4:5380d574-a61c-450b-9d80-55fd2be49de1:0	Variable	x_i	2025-11-30T08:15:24.654000000Z	x_i	tex_extractor	['We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.']					
4:5380d574-a61c-450b-9d80-55fd2be49de1:44	Equation		2025-11-30T08:15:31.161000000Z		tex_extractor		3206	inline_fragment	\ell(x_i, y_i; \theta)	85fe9e97f073eeffcce286a5a2484571ce6f2494	4020
4:5380d574-a61c-450b-9d80-55fd2be49de1:5	Variable	\theta	2025-11-30T08:15:25.232000000Z	\theta	tex_extractor	['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']					
4:5380d574-a61c-450b-9d80-55fd2be49de1:49	Equation		2025-11-30T08:15:32.340000000Z		tex_extractor		8269	inline_fragment	\mathcal{J}(\theta)	fdf38a4361746c7bf445f8aa8f570483b160f3b5	9083
4:5380d574-a61c-450b-9d80-55fd2be49de1:48	Equation		2025-11-30T08:15:32.151000000Z		tex_extractor		7924	inline_equation	\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)	27ba3d6cc5be0c81918dc032618596f7dba82917	8738
4:5380d574-a61c-450b-9d80-55fd2be49de1:17	Variable	g_t	2025-11-30T08:15:26.429000000Z	g_t	tex_extractor	['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']					
4:5380d574-a61c-450b-9d80-55fd2be49de1:47	Equation		2025-11-30T08:15:32.048000000Z		tex_extractor		6820	inline_fragment	\nabla \mathcal{L}(\theta_t)	aaf77515d8d096e1d3e5def4f7cb626b7b934c4a	7634
4:5380d574-a61c-450b-9d80-55fd2be49de1:46	Equation		2025-11-30T08:15:31.749000000Z		tex_extractor		3728	inline_equation	-\nabla \mathcal{L}(\theta)	3373829644d3b668b0865dbfd0f6fa1d82cd2912	4542
4:5380d574-a61c-450b-9d80-55fd2be49de1:45	Equation		2025-11-30T08:15:31.615000000Z		tex_extractor		3604	inline_fragment	\nabla \mathcal{L}(\theta)	e703e0e323865e4194c013fb99ea6c750b83cdb1	4418
4:5380d574-a61c-450b-9d80-55fd2be49de1:43	Equation		2025-11-30T08:15:31.014000000Z		tex_extractor		3141	inline_fragment	\mathcal{L}(\theta)	151d0c9f50bb83e110c61e00ebc661ee3bad9446	3955
4:5380d574-a61c-450b-9d80-55fd2be49de1:28	Equation		2025-11-30T08:15:27.388000000Z		tex_extractor		3330	block	\begin{equation} \label{eq:empirical_risk} \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(x_i, y_i; \theta) \end{equation}	f35bae0683432f536ecb2c9733b87e2d44206e0b	4144
4:5380d574-a61c-450b-9d80-55fd2be49de1:38	Equation		2025-11-30T08:15:29.907000000Z		tex_extractor		10845	block	\begin{align} \label{eq:adam_raw} v_t &= \beta_1 v_{t-1} + (1 - \beta_1) g_t \nonumber \\ s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \end{align}	0c8dbdec1eded5b7d42a00d6429e905c1783e3a3	11659
4:5380d574-a61c-450b-9d80-55fd2be49de1:36	Equation		2025-11-30T08:15:29.462000000Z		tex_extractor		9685	block	\begin{equation} \label{eq:momentum_velocity} v_{t+1} = \beta v_t + \eta g_t \end{equation}	0f6bfb5b63ba5edb2526160ab8e90c018dfe08e6	10499
4:5380d574-a61c-450b-9d80-55fd2be49de1:34	Equation		2025-11-30T08:15:28.823000000Z		tex_extractor		7746	block	\begin{equation} \label{eq:sgd_update} \theta_{t+1} = \theta_t - \eta g_t \end{equation}	1eca8997523ea67861f71828e524e0b1481c05f3	8560
4:5380d574-a61c-450b-9d80-55fd2be49de1:33	Equation		2025-11-30T08:15:28.427000000Z		tex_extractor		7402	block	\begin{equation} \label{eq:stochastic_grad} g_t = \nabla_\theta \left( \frac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i, y_i; \theta_t) \right) \end{equation}	247533654e8b4f2ba01161173875f405cc11ac50	8216
4:5380d574-a61c-450b-9d80-55fd2be49de1:29	Equation		2025-11-30T08:15:27.711000000Z		tex_extractor		4111	block	\begin{equation} \label{eq:regularized_loss} \mathcal{J}(\theta) = \mathcal{L}(\theta) + \frac{\lambda}{2} \|\theta\|^2 \end{equation}	1db931645a0220170ad4c7720de4df4140a41c6d	4925
4:5380d574-a61c-450b-9d80-55fd2be49de1:30	Equation		2025-11-30T08:15:27.883000000Z		tex_extractor		5029	block	\begin{equation} \label{eq:lipschitz} \| \nabla \mathcal{L}(\theta_1) - \nabla \mathcal{L}(\theta_2) \| \le L \| \theta_1 - \theta_2 \| \end{equation}	54cfb611355f7e983999607b164e63e538161d1b	5843
4:5380d574-a61c-450b-9d80-55fd2be49de1:31	Equation		2025-11-30T08:15:28.033000000Z		tex_extractor		5548	block	\begin{equation} \label{eq:strong_convexity} \mathcal{L}(\theta_2) \ge \mathcal{L}(\theta_1) + \nabla \mathcal{L}(\theta_1)^T (\theta_2 - \theta_1) + \frac{\mu}{2} \| \theta_2 - \theta_1 \|^2 \end{equation}	6a216f8cb9b4f7891ad320f480201997bb9a0b6f	6362
4:5380d574-a61c-450b-9d80-55fd2be49de1:32	Equation		2025-11-30T08:15:28.224000000Z		tex_extractor		6667	block	\begin{equation} \label{eq:gd_update} \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}	4cc7753bf24e7b3fcab1e6102a34287ac1f22aa9	7481
4:5380d574-a61c-450b-9d80-55fd2be49de1:35	Equation		2025-11-30T08:15:29.202000000Z		tex_extractor		8572	block	\begin{equation} \label{eq:weight_decay_update} \theta_{t+1} = \theta_t - \eta \left( \nabla \mathcal{L}(\theta_t) + \lambda \theta_t \right) = (1 - \eta \lambda)\theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}	c008450bf6dbbf56567c7e67efd0f559921c7aa3	9386
4:5380d574-a61c-450b-9d80-55fd2be49de1:37	Equation		2025-11-30T08:15:29.774000000Z		tex_extractor		9915	block	\begin{equation} \label{eq:momentum_update} \theta_{t+1} = \theta_t - v_{t+1} \end{equation}	4aa9a06e442a17485860371a01a1ce4bb1f65dcd	10729
4:5380d574-a61c-450b-9d80-55fd2be49de1:39	Equation		2025-11-30T08:15:30.165000000Z		tex_extractor		11559	block	\begin{equation} \label{eq:adam_update} \theta_{t+1} = \theta_t - \eta \frac{\hat{v}_t}{\sqrt{\hat{s}_t} + \epsilon} \end{equation}	fa87468d37a2a2f142661c7ebb7c46d0e451b907	12373
4:5380d574-a61c-450b-9d80-55fd2be49de1:40	Equation		2025-11-30T08:15:30.424000000Z		tex_extractor		12769	block	\begin{align} \label{eq:error_expansion} \| \theta_{t+1} - \theta^* \|^2 &= \| \theta_t - \eta \nabla \mathcal{L}(\theta_t) - \theta^* \|^2 \nonumber \\ &= \| \theta_t - \theta^* \|^2 - 2\eta \nabla \mathcal{L}(\theta_t)^T (\theta_t - \theta^*) + \eta^2 \| \nabla \mathcal{L}(\theta_t) \|^2 \end{align}	095ec6b298643b6e0b862c2db3501cd16b29c85d	13583
4:5380d574-a61c-450b-9d80-55fd2be49de1:41	Equation		2025-11-30T08:15:30.595000000Z		tex_extractor		13938	block	\begin{equation} \label{eq:linear_contraction} \| \theta_{t+1} - \theta^* \|^2 \le \left( 1 - \frac{\mu}{L} \right) \| \theta_t - \theta^* \|^2 \end{equation}	e713652eec7e3e41c4fa4bcc0abbd3bd52bdc459	14752
4:5380d574-a61c-450b-9d80-55fd2be49de1:42	Equation		2025-11-30T08:15:30.817000000Z		tex_extractor		14342	block	\begin{equation} \label{eq:convergence_rate} \| \theta_t - \theta^* \|^2 \le \left( 1 - \frac{\mu}{L} \right)^t \| \theta_0 - \theta^* \|^2 \end{equation}	6b72669b2596f3ab31b98b75b9a642f66620ef96	15156
4:5380d574-a61c-450b-9d80-55fd2be49de1:50	Equation		2025-11-30T08:15:32.683000000Z		tex_extractor		13211	inline_equation	\eta \le 1/L	2f899194b0b0515513500195876ed8f9a95e65d1	14025
4:5380d574-a61c-450b-9d80-55fd2be49de1:51	Equation		2025-11-30T08:15:32.794000000Z		tex_extractor		13814	inline_equation	\eta = \frac{1}{L}	f3601560cead6e76e028a3c6a8c7d70be6a9f4d9	14628
4:5380d574-a61c-450b-9d80-55fd2be49de1:52	Equation		2025-11-30T08:15:32.881000000Z		tex_extractor		14102	inline_equation	\kappa = L/\mu	70881c54403bc5198da1dd721eb0f89ed3ff4e4e	14916
4:5380d574-a61c-450b-9d80-55fd2be49de1:53	Equation		2025-11-30T08:15:32.968000000Z		tex_extractor		14168	inline_equation	(1 - \mu/L)	d6cb54c89b72011ef3940e2f5ec0dc2a7390e360	14982
4:5380d574-a61c-450b-9d80-55fd2be49de1:54	Equation		2025-11-30T08:15:33.103000000Z		tex_extractor		15296	inline_equation	O(1/\sqrt{t})	811d905e2c67362f3a156aa75949a9b7bebf9633	16110


=== VARIABLE-EQUATION RELATIONSHIPS ===
~id	~start_node_id	~end_node_id	~relationship_type	first_seen	context_before	relationship_type	sentences	count	context_after	last_seen
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081900	4:5380d574-a61c-450b-9d80-55fd2be49de1:0	4:5380d574-a61c-450b-9d80-55fd2be49de1:44	APPEARS_IN	2025-11-30T08:15:31.161000000Z	['\section{Mathematical Preliminari']	equation_internal	['Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']	1	['cific update rules','we must rigorously define the problem setting and the variables governing the optimization landscape.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:1168684103302643717	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:44	APPEARS_IN	2025-11-30T08:15:31.384000000Z	['\section{Mathematical Preliminari']	equation_internal	['Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']	1	['cific update rules','we must rigorously define the problem setting and the variables governing the optimization landscape.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081905	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:49	APPEARS_IN	2025-11-30T08:15:32.340000000Z	['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','\begin{equation} \label{eq:stochastic_grad} g_t = \na']	equation_internal	['If we return to the regularized loss $\mathcal{J}(\theta)$ defined in Equation (\ref{eq:regularized_loss})','which included the parameter $\lambda$','the gradient update changes slightly. The gradient of the regularization term $\frac{\lambda}{2}\|\theta\|^2$ is simply $\lambda \theta$. Thus','the update rule incorporating weight decay is:']	1	['ac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i','y_i; \theta_t) \right) \end{equation}','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6919780827454767152	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:48	APPEARS_IN	2025-11-30T08:15:32.254000000Z	['\subsection{Stochastic Gradient Descent (SGD)}','To address the computational bottleneck','we approximate the full gradient using a single sample o']	equation_internal	['Ideally','the stochastic gradient is an unbiased estimator of the true gradient','meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However','the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5','this noise prevents exact convergence unless the learning rate $\eta$ decays over time.']	1	['size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081904	4:5380d574-a61c-450b-9d80-55fd2be49de1:17	4:5380d574-a61c-450b-9d80-55fd2be49de1:48	APPEARS_IN	2025-11-30T08:15:32.151000000Z	['\subsection{Stochastic Gradient Descent (SGD)}','To address the computational bottleneck','we approximate the full gradient using a single sample o']	equation_internal	['Ideally','the stochastic gradient is an unbiased estimator of the true gradient','meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However','the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5','this noise prevents exact convergence unless the learning rate $\eta$ decays over time.']	1	['size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081903	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:47	APPEARS_IN	2025-11-30T08:15:32.048000000Z	['\section{Gradient Descent Methods}','With the objective function $\mathcal{L}(\theta)$ and g']	equation_internal	['While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally prohibitive for large datasets.']	1	['efined in the previous section','we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081902	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:46	APPEARS_IN	2025-11-30T08:15:31.749000000Z	['We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.','The model is parameterized by a weight vector. We define this vector as $\the']	equation_internal	['To perform optimization','we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently','$-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.']	1	['d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.','Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081901	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:45	APPEARS_IN	2025-11-30T08:15:31.615000000Z	['\subsection{Problem Formulation}','We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','']	equation_internal	['To perform optimization','we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently','$-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.']	1	['responding label.','The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081899	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:43	APPEARS_IN	2025-11-30T08:15:31.014000000Z	['\subsection{Implications']	equation_internal	['The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.','Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:','Throughout the remainder of this paper','unless specified otherwise','we will focus on minimizing $\mathcal{L}(\theta)$','assuming $\lambda=0$ for simplicity','though the derivations easily extend to the regularized case.','With the objective function $\mathcal{L}(\theta)$ and gradient $\nabla \mathcal{L}$ defined in the previous section','we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.','Let $\theta^*$ denote the global minimizer of the loss function $\mathcal{L}(\theta)$. Our goal is to bound the squared distance error $\|\theta_t - \theta^*\|^2$ as $t \rightarrow \infty$.','This paper has reviewed the mathematical foundations and algorithmic developments in optimization for machine learning. We started by defining the empirical risk $\mathcal{L}(\theta)$ over a dataset $\mathcal{D}$ and explored the geometry of this landscape through the Lipschitz constant $L$ and strong convexity parameter $\mu$.']	6	['}','Recall the Adam update in Equation (\ref{eq:adam_update}). While Adam generally converges faster empirically on non-convex problems (like neural networks)','proving its convergence in the convex setting is subtle. In fact','for certain definitions of $\beta_1','\beta_2$','Adam can fail to converge even on convex problems.']	2025-11-30T08:15:33.236000000Z
5:5380d574-a61c-450b-9d80-55fd2be49de1:1152921504606846981	4:5380d574-a61c-450b-9d80-55fd2be49de1:5	4:5380d574-a61c-450b-9d80-55fd2be49de1:28	APPEARS_IN	2025-11-30T08:15:27.624000000Z	['\section{Mathematical Preliminaries}','Before deriving specific update rules','we must rigorously define the problem setting and the variables governing the op']	equation_internal	[]	1	['\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.','The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:1159676904047902737	4:5380d574-a61c-450b-9d80-55fd2be49de1:17	4:5380d574-a61c-450b-9d80-55fd2be49de1:38	APPEARS_IN	2025-11-30T08:15:29.993000000Z	['\begin{equation} \label{eq:momentum_update} \theta_{t+1} = \theta_t - v_{t+1} \end{equation}','This approach dampens']	equation_internal	[]	1	['$v_t$ accelerates.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:1157425104234217489	4:5380d574-a61c-450b-9d80-55fd2be49de1:17	4:5380d574-a61c-450b-9d80-55fd2be49de1:36	APPEARS_IN	2025-11-30T08:15:29.688000000Z	['\begin{equation} \label{eq:weight_decay_update} \theta_{t+1} = \theta_t - \eta \left( \nabla \mathcal{L}(\theta_t) + \lambda \theta_t \right) = (1 - \eta \lambda)\theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}','This formulation explicitly shows how $\lambda$ acts to decay the weights at']	equation_internal	[]	1	['Methods}','Standard SGD often struggles in areas where the surface curves much more steeply in one dimension than in another (common in local optima). To mitigate this','we introduce the concept of 'momentum','' which accumulates a velocity vector from past gradients.']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:1155173304420532241	4:5380d574-a61c-450b-9d80-55fd2be49de1:17	4:5380d574-a61c-450b-9d80-55fd2be49de1:34	APPEARS_IN	2025-11-30T08:15:29.031000000Z	['\begin{equation} \label{eq:gd_update} \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}','While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally']	equation_internal	[]	1	['ess the computational bottleneck','we approximate the full gradient using a single sample or a subset of samples. Let $B$ denote the batch size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']	
5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081889	4:5380d574-a61c-450b-9d80-55fd2be49de1:17	4:5380d574-a61c-450b-9d80-55fd2be49de1:33	APPEARS_IN	2025-11-30T08:15:28.427000000Z	['The most fundamental optimization algorithm is Gradient Descent. We introduce the iteration counter $t$','representing the discrete time step of the algorithm. We also define the learning rate $\eta$ (sometimes denoted as $\alpha$)','which is a positive scalar determining the step size.','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in S']	equation_internal	[]	1	['athcal{L}(\theta_t) \end{equation}','While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally prohibitive for large datasets.']	

=== END OF DOCUMENT KNOWLEDGE ===

You MUST:
1. ONLY use information from the extracted knowledge above
2. Cite specific sections/equations when answering
3. If something is not in the extracted knowledge, say "This is not covered in your study material"

You MUST output your response in the following JSON format:
{
  "thinking_process": "Your step-by-step reasoning",
  "sources_used": ["List of specific nodes/equations you referenced"],
  "grounding_citations": ["Section X.Y", "Equation Z"],
  "answer": "Your response to the student",
  "confidence": "high/medium/low"
}

IMPORTANT: Use Socratic method. Guide the student toward understanding rather than just giving answers. Reference their study material directly. DO NOT REVEAL THE ANSWERS DIRECTLY!
