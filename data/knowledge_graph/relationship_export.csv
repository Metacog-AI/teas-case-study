"~id","~start_node_id","~end_node_id","~relationship_type","first_seen","context_before","relationship_type","sentences","count","context_after","last_seen"
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081900","4:5380d574-a61c-450b-9d80-55fd2be49de1:0","4:5380d574-a61c-450b-9d80-55fd2be49de1:44","APPEARS_IN","2025-11-30T08:15:31.161000000Z","['\section{Mathematical Preliminari']","equation_internal","['Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']",1,"['cific update rules','we must rigorously define the problem setting and the variables governing the optimization landscape.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:1168684103302643717","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:44","APPEARS_IN","2025-11-30T08:15:31.384000000Z","['\section{Mathematical Preliminari']","equation_internal","['Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']",1,"['cific update rules','we must rigorously define the problem setting and the variables governing the optimization landscape.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081905","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:49","APPEARS_IN","2025-11-30T08:15:32.340000000Z","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','\begin{equation} \label{eq:stochastic_grad} g_t = \na']","equation_internal","['If we return to the regularized loss $\mathcal{J}(\theta)$ defined in Equation (\ref{eq:regularized_loss})','which included the parameter $\lambda$','the gradient update changes slightly. The gradient of the regularization term $\frac{\lambda}{2}\|\theta\|^2$ is simply $\lambda \theta$. Thus','the update rule incorporating weight decay is:']",1,"['ac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i','y_i; \theta_t) \right) \end{equation}','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6919780827454767152","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:48","APPEARS_IN","2025-11-30T08:15:32.254000000Z","['\subsection{Stochastic Gradient Descent (SGD)}','To address the computational bottleneck','we approximate the full gradient using a single sample o']","equation_internal","['Ideally','the stochastic gradient is an unbiased estimator of the true gradient','meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However','the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5','this noise prevents exact convergence unless the learning rate $\eta$ decays over time.']",1,"['size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081904","4:5380d574-a61c-450b-9d80-55fd2be49de1:17","4:5380d574-a61c-450b-9d80-55fd2be49de1:48","APPEARS_IN","2025-11-30T08:15:32.151000000Z","['\subsection{Stochastic Gradient Descent (SGD)}','To address the computational bottleneck','we approximate the full gradient using a single sample o']","equation_internal","['Ideally','the stochastic gradient is an unbiased estimator of the true gradient','meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However','the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5','this noise prevents exact convergence unless the learning rate $\eta$ decays over time.']",1,"['size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081903","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:47","APPEARS_IN","2025-11-30T08:15:32.048000000Z","['\section{Gradient Descent Methods}','With the objective function $\mathcal{L}(\theta)$ and g']","equation_internal","['While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally prohibitive for large datasets.']",1,"['efined in the previous section','we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081902","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:46","APPEARS_IN","2025-11-30T08:15:31.749000000Z","['We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.','The model is parameterized by a weight vector. We define this vector as $\the']","equation_internal","['To perform optimization','we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently','$-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.']",1,"['d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.','Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081901","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:45","APPEARS_IN","2025-11-30T08:15:31.615000000Z","['\subsection{Problem Formulation}','We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','']","equation_internal","['To perform optimization','we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently','$-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.']",1,"['responding label.','The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081899","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:43","APPEARS_IN","2025-11-30T08:15:31.014000000Z","['\subsection{Implications']","equation_internal","['The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.','Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:','Throughout the remainder of this paper','unless specified otherwise','we will focus on minimizing $\mathcal{L}(\theta)$','assuming $\lambda=0$ for simplicity','though the derivations easily extend to the regularized case.','With the objective function $\mathcal{L}(\theta)$ and gradient $\nabla \mathcal{L}$ defined in the previous section','we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.','Let $\theta^*$ denote the global minimizer of the loss function $\mathcal{L}(\theta)$. Our goal is to bound the squared distance error $\|\theta_t - \theta^*\|^2$ as $t \rightarrow \infty$.','This paper has reviewed the mathematical foundations and algorithmic developments in optimization for machine learning. We started by defining the empirical risk $\mathcal{L}(\theta)$ over a dataset $\mathcal{D}$ and explored the geometry of this landscape through the Lipschitz constant $L$ and strong convexity parameter $\mu$.']",6,"['}','Recall the Adam update in Equation (\ref{eq:adam_update}). While Adam generally converges faster empirically on non-convex problems (like neural networks)','proving its convergence in the convex setting is subtle. In fact','for certain definitions of $\beta_1','\beta_2$','Adam can fail to converge even on convex problems.']","2025-11-30T08:15:33.236000000Z"
"5:5380d574-a61c-450b-9d80-55fd2be49de1:1152921504606846981","4:5380d574-a61c-450b-9d80-55fd2be49de1:5","4:5380d574-a61c-450b-9d80-55fd2be49de1:28","APPEARS_IN","2025-11-30T08:15:27.624000000Z","['\section{Mathematical Preliminaries}','Before deriving specific update rules','we must rigorously define the problem setting and the variables governing the op']","equation_internal","[]",1,"['\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.','The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:1159676904047902737","4:5380d574-a61c-450b-9d80-55fd2be49de1:17","4:5380d574-a61c-450b-9d80-55fd2be49de1:38","APPEARS_IN","2025-11-30T08:15:29.993000000Z","['\begin{equation} \label{eq:momentum_update} \theta_{t+1} = \theta_t - v_{t+1} \end{equation}','This approach dampens']","equation_internal","[]",1,"['$v_t$ accelerates.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:1157425104234217489","4:5380d574-a61c-450b-9d80-55fd2be49de1:17","4:5380d574-a61c-450b-9d80-55fd2be49de1:36","APPEARS_IN","2025-11-30T08:15:29.688000000Z","['\begin{equation} \label{eq:weight_decay_update} \theta_{t+1} = \theta_t - \eta \left( \nabla \mathcal{L}(\theta_t) + \lambda \theta_t \right) = (1 - \eta \lambda)\theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}','This formulation explicitly shows how $\lambda$ acts to decay the weights at']","equation_internal","[]",1,"['Methods}','Standard SGD often struggles in areas where the surface curves much more steeply in one dimension than in another (common in local optima). To mitigate this','we introduce the concept of 'momentum','' which accumulates a velocity vector from past gradients.']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:1155173304420532241","4:5380d574-a61c-450b-9d80-55fd2be49de1:17","4:5380d574-a61c-450b-9d80-55fd2be49de1:34","APPEARS_IN","2025-11-30T08:15:29.031000000Z","['\begin{equation} \label{eq:gd_update} \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}','While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally']","equation_internal","[]",1,"['ess the computational bottleneck','we approximate the full gradient using a single sample or a subset of samples. Let $B$ denote the batch size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']",
"5:5380d574-a61c-450b-9d80-55fd2be49de1:6917529027641081889","4:5380d574-a61c-450b-9d80-55fd2be49de1:17","4:5380d574-a61c-450b-9d80-55fd2be49de1:33","APPEARS_IN","2025-11-30T08:15:28.427000000Z","['The most fundamental optimization algorithm is Gradient Descent. We introduce the iteration counter $t$','representing the discrete time step of the algorithm. We also define the learning rate $\eta$ (sometimes denoted as $\alpha$)','which is a positive scalar determining the step size.','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in S']","equation_internal","[]",1,"['athcal{L}(\theta_t) \end{equation}','While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally prohibitive for large datasets.']",