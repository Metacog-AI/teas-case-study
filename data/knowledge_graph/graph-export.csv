"~start_node_id","~start_node_labels","~start_node_property_latex_sample","~start_node_property_created","~start_node_property_name","~start_node_property_source","~start_node_property_paragraph_samples","~relationship_type","~relationship_property_first_seen","~relationship_property_context_before","~relationship_property_relationship_type","~relationship_property_sentences","~relationship_property_count","~relationship_property_context_after","~end_node_id","~end_node_labels","~end_node_property_pos","~end_node_property_created","~end_node_property_source","~end_node_property_type","~end_node_property_latex","~end_node_property_hash","~end_node_property_abs_pos","~relationship_property_last_seen"
"4:5380d574-a61c-450b-9d80-55fd2be49de1:0","Variable","x_i","2025-11-30T08:15:24.654000000Z","x_i","tex_extractor","['We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.']","APPEARS_IN","2025-11-30T08:15:31.161000000Z","['\section{Mathematical Preliminari']","equation_internal","['Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']",1,"['cific update rules','we must rigorously define the problem setting and the variables governing the optimization landscape.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:44","Equation",3206,"2025-11-30T08:15:31.161000000Z","tex_extractor","inline_fragment","\ell(x_i, y_i; \theta)","85fe9e97f073eeffcce286a5a2484571ce6f2494",4020,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:31.384000000Z","['\section{Mathematical Preliminari']","equation_internal","['Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']",1,"['cific update rules','we must rigorously define the problem setting and the variables governing the optimization landscape.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:44","Equation",3206,"2025-11-30T08:15:31.161000000Z","tex_extractor","inline_fragment","\ell(x_i, y_i; \theta)","85fe9e97f073eeffcce286a5a2484571ce6f2494",4020,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:32.340000000Z","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','\begin{equation} \label{eq:stochastic_grad} g_t = \na']","equation_internal","['If we return to the regularized loss $\mathcal{J}(\theta)$ defined in Equation (\ref{eq:regularized_loss})','which included the parameter $\lambda$','the gradient update changes slightly. The gradient of the regularization term $\frac{\lambda}{2}\|\theta\|^2$ is simply $\lambda \theta$. Thus','the update rule incorporating weight decay is:']",1,"['ac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i','y_i; \theta_t) \right) \end{equation}','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:']","4:5380d574-a61c-450b-9d80-55fd2be49de1:49","Equation",8269,"2025-11-30T08:15:32.340000000Z","tex_extractor","inline_fragment","\mathcal{J}(\theta)","fdf38a4361746c7bf445f8aa8f570483b160f3b5",9083,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:32.254000000Z","['\subsection{Stochastic Gradient Descent (SGD)}','To address the computational bottleneck','we approximate the full gradient using a single sample o']","equation_internal","['Ideally','the stochastic gradient is an unbiased estimator of the true gradient','meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However','the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5','this noise prevents exact convergence unless the learning rate $\eta$ decays over time.']",1,"['size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']","4:5380d574-a61c-450b-9d80-55fd2be49de1:48","Equation",7924,"2025-11-30T08:15:32.151000000Z","tex_extractor","inline_equation","\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)","27ba3d6cc5be0c81918dc032618596f7dba82917",8738,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:17","Variable","g_t","2025-11-30T08:15:26.429000000Z","g_t","tex_extractor","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']","APPEARS_IN","2025-11-30T08:15:32.151000000Z","['\subsection{Stochastic Gradient Descent (SGD)}','To address the computational bottleneck','we approximate the full gradient using a single sample o']","equation_internal","['Ideally','the stochastic gradient is an unbiased estimator of the true gradient','meaning $\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)$. However','the variance of this estimator introduces noise into the optimization trajectory. As we will discuss in Section 5','this noise prevents exact convergence unless the learning rate $\eta$ decays over time.']",1,"['size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']","4:5380d574-a61c-450b-9d80-55fd2be49de1:48","Equation",7924,"2025-11-30T08:15:32.151000000Z","tex_extractor","inline_equation","\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)","27ba3d6cc5be0c81918dc032618596f7dba82917",8738,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:32.048000000Z","['\section{Gradient Descent Methods}','With the objective function $\mathcal{L}(\theta)$ and g']","equation_internal","['While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally prohibitive for large datasets.']",1,"['efined in the previous section','we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:47","Equation",6820,"2025-11-30T08:15:32.048000000Z","tex_extractor","inline_fragment","\nabla \mathcal{L}(\theta_t)","aaf77515d8d096e1d3e5def4f7cb626b7b934c4a",7634,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:31.749000000Z","['We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.','The model is parameterized by a weight vector. We define this vector as $\the']","equation_internal","['To perform optimization','we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently','$-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.']",1,"['d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.','Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:']","4:5380d574-a61c-450b-9d80-55fd2be49de1:46","Equation",3728,"2025-11-30T08:15:31.749000000Z","tex_extractor","inline_equation","-\nabla \mathcal{L}(\theta)","3373829644d3b668b0865dbfd0f6fa1d82cd2912",4542,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:31.615000000Z","['\subsection{Problem Formulation}','We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','']","equation_internal","['To perform optimization','we require the derivative of this loss with respect to the parameters. We denote the gradient of the loss function as $\nabla \mathcal{L}(\theta)$. This vector points in the direction of the steepest ascent of the loss surface. Consequently','$-\nabla \mathcal{L}(\theta)$ points in the direction of steepest descent.']",1,"['responding label.','The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:45","Equation",3604,"2025-11-30T08:15:31.615000000Z","tex_extractor","inline_fragment","\nabla \mathcal{L}(\theta)","e703e0e323865e4194c013fb99ea6c750b83cdb1",4418,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:31.014000000Z","['\subsection{Implications']","equation_internal","['The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.','Let $\ell(x_i','y_i; \theta)$ be the per-sample loss function. The empirical risk $\mathcal{L}(\theta)$ is formally defined as:','Throughout the remainder of this paper','unless specified otherwise','we will focus on minimizing $\mathcal{L}(\theta)$','assuming $\lambda=0$ for simplicity','though the derivations easily extend to the regularized case.','With the objective function $\mathcal{L}(\theta)$ and gradient $\nabla \mathcal{L}$ defined in the previous section','we can now formulate the iterative algorithms used to find the optimal parameters $\theta^*$.','Let $\theta^*$ denote the global minimizer of the loss function $\mathcal{L}(\theta)$. Our goal is to bound the squared distance error $\|\theta_t - \theta^*\|^2$ as $t \rightarrow \infty$.','This paper has reviewed the mathematical foundations and algorithmic developments in optimization for machine learning. We started by defining the empirical risk $\mathcal{L}(\theta)$ over a dataset $\mathcal{D}$ and explored the geometry of this landscape through the Lipschitz constant $L$ and strong convexity parameter $\mu$.']",6,"['}','Recall the Adam update in Equation (\ref{eq:adam_update}). While Adam generally converges faster empirically on non-convex problems (like neural networks)','proving its convergence in the convex setting is subtle. In fact','for certain definitions of $\beta_1','\beta_2$','Adam can fail to converge even on convex problems.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:43","Equation",3141,"2025-11-30T08:15:31.014000000Z","tex_extractor","inline_fragment","\mathcal{L}(\theta)","151d0c9f50bb83e110c61e00ebc661ee3bad9446",3955,"2025-11-30T08:15:33.236000000Z"
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']","APPEARS_IN","2025-11-30T08:15:27.624000000Z","['\section{Mathematical Preliminaries}','Before deriving specific update rules','we must rigorously define the problem setting and the variables governing the op']","equation_internal","[]",1,"['\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.','The model is parameterized by a weight vector. We define this vector as $\theta \in \mathbb{R}^d$','where $d$ represents the dimensionality of the parameter space. The goal is to minimize a discrepancy between the model predictions and the true labels. We define the objective function (or loss function) $\mathcal{L}(\theta)$ as the average loss over the dataset.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:28","Equation",3330,"2025-11-30T08:15:27.388000000Z","tex_extractor","block","\begin{equation} \label{eq:empirical_risk} \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(x_i, y_i; \theta) \end{equation}","f35bae0683432f536ecb2c9733b87e2d44206e0b",4144,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:17","Variable","g_t","2025-11-30T08:15:26.429000000Z","g_t","tex_extractor","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']","APPEARS_IN","2025-11-30T08:15:29.993000000Z","['\begin{equation} \label{eq:momentum_update} \theta_{t+1} = \theta_t - v_{t+1} \end{equation}','This approach dampens']","equation_internal","[]",1,"['$v_t$ accelerates.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:38","Equation",10845,"2025-11-30T08:15:29.907000000Z","tex_extractor","block","\begin{align} \label{eq:adam_raw} v_t &= \beta_1 v_{t-1} + (1 - \beta_1) g_t \nonumber \\ s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \end{align}","0c8dbdec1eded5b7d42a00d6429e905c1783e3a3",11659,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:17","Variable","g_t","2025-11-30T08:15:26.429000000Z","g_t","tex_extractor","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']","APPEARS_IN","2025-11-30T08:15:29.688000000Z","['\begin{equation} \label{eq:weight_decay_update} \theta_{t+1} = \theta_t - \eta \left( \nabla \mathcal{L}(\theta_t) + \lambda \theta_t \right) = (1 - \eta \lambda)\theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}','This formulation explicitly shows how $\lambda$ acts to decay the weights at']","equation_internal","[]",1,"['Methods}','Standard SGD often struggles in areas where the surface curves much more steeply in one dimension than in another (common in local optima). To mitigate this','we introduce the concept of 'momentum','' which accumulates a velocity vector from past gradients.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:36","Equation",9685,"2025-11-30T08:15:29.462000000Z","tex_extractor","block","\begin{equation} \label{eq:momentum_velocity} v_{t+1} = \beta v_t + \eta g_t \end{equation}","0f6bfb5b63ba5edb2526160ab8e90c018dfe08e6",10499,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:17","Variable","g_t","2025-11-30T08:15:26.429000000Z","g_t","tex_extractor","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']","APPEARS_IN","2025-11-30T08:15:29.031000000Z","['\begin{equation} \label{eq:gd_update} \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}','While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally']","equation_internal","[]",1,"['ess the computational bottleneck','we approximate the full gradient using a single sample or a subset of samples. Let $B$ denote the batch size. In standard SGD','$B=1$','while in Mini-batch SGD','$1 < B < N$.','Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:']","4:5380d574-a61c-450b-9d80-55fd2be49de1:34","Equation",7746,"2025-11-30T08:15:28.823000000Z","tex_extractor","block","\begin{equation} \label{eq:sgd_update} \theta_{t+1} = \theta_t - \eta g_t \end{equation}","1eca8997523ea67861f71828e524e0b1481c05f3",8560,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:17","Variable","g_t","2025-11-30T08:15:26.429000000Z","g_t","tex_extractor","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']","APPEARS_IN","2025-11-30T08:15:28.427000000Z","['The most fundamental optimization algorithm is Gradient Descent. We introduce the iteration counter $t$','representing the discrete time step of the algorithm. We also define the learning rate $\eta$ (sometimes denoted as $\alpha$)','which is a positive scalar determining the step size.','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in S']","equation_internal","[]",1,"['athcal{L}(\theta_t) \end{equation}','While theoretically sound','calculating $\nabla \mathcal{L}(\theta_t)$ requires summing over all $N$ samples in $\mathcal{D}$','which is computationally prohibitive for large datasets.']","4:5380d574-a61c-450b-9d80-55fd2be49de1:33","Equation",7402,"2025-11-30T08:15:28.427000000Z","tex_extractor","block","\begin{equation} \label{eq:stochastic_grad} g_t = \nabla_\theta \left( \frac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i, y_i; \theta_t) \right) \end{equation}","247533654e8b4f2ba01161173875f405cc11ac50",8216,