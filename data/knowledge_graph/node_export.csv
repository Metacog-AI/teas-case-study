"~id","~labels","latex_sample","created","name","source","paragraph_samples","pos","type","latex","hash","abs_pos"
"4:5380d574-a61c-450b-9d80-55fd2be49de1:0","Variable","x_i","2025-11-30T08:15:24.654000000Z","x_i","tex_extractor","['We assume a supervised learning setting. Let the dataset be denoted by $\mathcal{D}$','consisting of $N$ pairs of inputs and targets. Specifically','we let $x_i$ represent the input feature vector for the $i$-th sample','and $y_i$ represent the corresponding label.']",,,,,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:44","Equation",,"2025-11-30T08:15:31.161000000Z",,"tex_extractor",,3206,"inline_fragment","\ell(x_i, y_i; \theta)","85fe9e97f073eeffcce286a5a2484571ce6f2494",4020
"4:5380d574-a61c-450b-9d80-55fd2be49de1:5","Variable","\theta","2025-11-30T08:15:25.232000000Z","\theta","tex_extractor","['To prevent overfitting','it is common to add a penalty term to the objective function. Let $\lambda$ denote the regularization coefficient','a non-negative hyperparameter. We typically use the $L_2$ norm of the weight vector $\theta$. The regularized objective function becomes:','In Batch Gradient Descent','we compute the gradient over the entire dataset $\mathcal{D}$ defined in Section 2. The update rule for the weight vector $\theta$ at iteration $t$ is:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','While momentum adapts the step direction','it uses a global learning rate $\eta$ for all parameters in $\theta$. Adaptive methods compute individual learning rates for different parameters. The most popular method is Adam.','Finally','we update the parameters $\theta$. We introduce a small scalar $\epsilon$ to prevent division by zero. Combining the learning rate $\eta$ with the adaptive moments:']",,,,,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:49","Equation",,"2025-11-30T08:15:32.340000000Z",,"tex_extractor",,8269,"inline_fragment","\mathcal{J}(\theta)","fdf38a4361746c7bf445f8aa8f570483b160f3b5",9083
"4:5380d574-a61c-450b-9d80-55fd2be49de1:48","Equation",,"2025-11-30T08:15:32.151000000Z",,"tex_extractor",,7924,"inline_equation","\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)","27ba3d6cc5be0c81918dc032618596f7dba82917",8738
"4:5380d574-a61c-450b-9d80-55fd2be49de1:17","Variable","g_t","2025-11-30T08:15:26.429000000Z","g_t","tex_extractor","['Let $\mathcal{B}_t$ be a random subset of indices from the dataset $\{1','\dots','N\}$ of size $B$ chosen at timestep $t$. We define the stochastic gradient estimate $g_t$ as:','The update rule for Mini-batch SGD substitutes the full gradient in Equation (\ref{eq:gd_update}) with the estimate $g_t$. Recall that $\eta$ is our learning rate. The update becomes:','Building on the stochastic gradient $g_t$ defined in Equation (\ref{eq:stochastic_grad}) and the parameters $\theta$ and $\eta$ from Section 3','the momentum update involves two steps. First','we update the velocity:','This approach dampens oscillations. If the gradient $g_t$ changes direction frequently (as in a ravine)','the terms in $v_t$ cancel out. If the direction is consistent','$v_t$ accelerates.','At iteration $t$','we compute the raw moments based on the gradient $g_t$:']",,,,,
"4:5380d574-a61c-450b-9d80-55fd2be49de1:47","Equation",,"2025-11-30T08:15:32.048000000Z",,"tex_extractor",,6820,"inline_fragment","\nabla \mathcal{L}(\theta_t)","aaf77515d8d096e1d3e5def4f7cb626b7b934c4a",7634
"4:5380d574-a61c-450b-9d80-55fd2be49de1:46","Equation",,"2025-11-30T08:15:31.749000000Z",,"tex_extractor",,3728,"inline_equation","-\nabla \mathcal{L}(\theta)","3373829644d3b668b0865dbfd0f6fa1d82cd2912",4542
"4:5380d574-a61c-450b-9d80-55fd2be49de1:45","Equation",,"2025-11-30T08:15:31.615000000Z",,"tex_extractor",,3604,"inline_fragment","\nabla \mathcal{L}(\theta)","e703e0e323865e4194c013fb99ea6c750b83cdb1",4418
"4:5380d574-a61c-450b-9d80-55fd2be49de1:43","Equation",,"2025-11-30T08:15:31.014000000Z",,"tex_extractor",,3141,"inline_fragment","\mathcal{L}(\theta)","151d0c9f50bb83e110c61e00ebc661ee3bad9446",3955
"4:5380d574-a61c-450b-9d80-55fd2be49de1:28","Equation",,"2025-11-30T08:15:27.388000000Z",,"tex_extractor",,3330,"block","\begin{equation} \label{eq:empirical_risk} \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(x_i, y_i; \theta) \end{equation}","f35bae0683432f536ecb2c9733b87e2d44206e0b",4144
"4:5380d574-a61c-450b-9d80-55fd2be49de1:38","Equation",,"2025-11-30T08:15:29.907000000Z",,"tex_extractor",,10845,"block","\begin{align} \label{eq:adam_raw} v_t &= \beta_1 v_{t-1} + (1 - \beta_1) g_t \nonumber \\ s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \end{align}","0c8dbdec1eded5b7d42a00d6429e905c1783e3a3",11659
"4:5380d574-a61c-450b-9d80-55fd2be49de1:36","Equation",,"2025-11-30T08:15:29.462000000Z",,"tex_extractor",,9685,"block","\begin{equation} \label{eq:momentum_velocity} v_{t+1} = \beta v_t + \eta g_t \end{equation}","0f6bfb5b63ba5edb2526160ab8e90c018dfe08e6",10499
"4:5380d574-a61c-450b-9d80-55fd2be49de1:34","Equation",,"2025-11-30T08:15:28.823000000Z",,"tex_extractor",,7746,"block","\begin{equation} \label{eq:sgd_update} \theta_{t+1} = \theta_t - \eta g_t \end{equation}","1eca8997523ea67861f71828e524e0b1481c05f3",8560
"4:5380d574-a61c-450b-9d80-55fd2be49de1:33","Equation",,"2025-11-30T08:15:28.427000000Z",,"tex_extractor",,7402,"block","\begin{equation} \label{eq:stochastic_grad} g_t = \nabla_\theta \left( \frac{1}{B} \sum_{i \in \mathcal{B}_t} \ell(x_i, y_i; \theta_t) \right) \end{equation}","247533654e8b4f2ba01161173875f405cc11ac50",8216
"4:5380d574-a61c-450b-9d80-55fd2be49de1:29","Equation",,"2025-11-30T08:15:27.711000000Z",,"tex_extractor",,4111,"block","\begin{equation} \label{eq:regularized_loss} \mathcal{J}(\theta) = \mathcal{L}(\theta) + \frac{\lambda}{2} \|\theta\|^2 \end{equation}","1db931645a0220170ad4c7720de4df4140a41c6d",4925
"4:5380d574-a61c-450b-9d80-55fd2be49de1:30","Equation",,"2025-11-30T08:15:27.883000000Z",,"tex_extractor",,5029,"block","\begin{equation} \label{eq:lipschitz} \| \nabla \mathcal{L}(\theta_1) - \nabla \mathcal{L}(\theta_2) \| \le L \| \theta_1 - \theta_2 \| \end{equation}","54cfb611355f7e983999607b164e63e538161d1b",5843
"4:5380d574-a61c-450b-9d80-55fd2be49de1:31","Equation",,"2025-11-30T08:15:28.033000000Z",,"tex_extractor",,5548,"block","\begin{equation} \label{eq:strong_convexity} \mathcal{L}(\theta_2) \ge \mathcal{L}(\theta_1) + \nabla \mathcal{L}(\theta_1)^T (\theta_2 - \theta_1) + \frac{\mu}{2} \| \theta_2 - \theta_1 \|^2 \end{equation}","6a216f8cb9b4f7891ad320f480201997bb9a0b6f",6362
"4:5380d574-a61c-450b-9d80-55fd2be49de1:32","Equation",,"2025-11-30T08:15:28.224000000Z",,"tex_extractor",,6667,"block","\begin{equation} \label{eq:gd_update} \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}","4cc7753bf24e7b3fcab1e6102a34287ac1f22aa9",7481
"4:5380d574-a61c-450b-9d80-55fd2be49de1:35","Equation",,"2025-11-30T08:15:29.202000000Z",,"tex_extractor",,8572,"block","\begin{equation} \label{eq:weight_decay_update} \theta_{t+1} = \theta_t - \eta \left( \nabla \mathcal{L}(\theta_t) + \lambda \theta_t \right) = (1 - \eta \lambda)\theta_t - \eta \nabla \mathcal{L}(\theta_t) \end{equation}","c008450bf6dbbf56567c7e67efd0f559921c7aa3",9386
"4:5380d574-a61c-450b-9d80-55fd2be49de1:37","Equation",,"2025-11-30T08:15:29.774000000Z",,"tex_extractor",,9915,"block","\begin{equation} \label{eq:momentum_update} \theta_{t+1} = \theta_t - v_{t+1} \end{equation}","4aa9a06e442a17485860371a01a1ce4bb1f65dcd",10729
"4:5380d574-a61c-450b-9d80-55fd2be49de1:39","Equation",,"2025-11-30T08:15:30.165000000Z",,"tex_extractor",,11559,"block","\begin{equation} \label{eq:adam_update} \theta_{t+1} = \theta_t - \eta \frac{\hat{v}_t}{\sqrt{\hat{s}_t} + \epsilon} \end{equation}","fa87468d37a2a2f142661c7ebb7c46d0e451b907",12373
"4:5380d574-a61c-450b-9d80-55fd2be49de1:40","Equation",,"2025-11-30T08:15:30.424000000Z",,"tex_extractor",,12769,"block","\begin{align} \label{eq:error_expansion} \| \theta_{t+1} - \theta^* \|^2 &= \| \theta_t - \eta \nabla \mathcal{L}(\theta_t) - \theta^* \|^2 \nonumber \\ &= \| \theta_t - \theta^* \|^2 - 2\eta \nabla \mathcal{L}(\theta_t)^T (\theta_t - \theta^*) + \eta^2 \| \nabla \mathcal{L}(\theta_t) \|^2 \end{align}","095ec6b298643b6e0b862c2db3501cd16b29c85d",13583
"4:5380d574-a61c-450b-9d80-55fd2be49de1:41","Equation",,"2025-11-30T08:15:30.595000000Z",,"tex_extractor",,13938,"block","\begin{equation} \label{eq:linear_contraction} \| \theta_{t+1} - \theta^* \|^2 \le \left( 1 - \frac{\mu}{L} \right) \| \theta_t - \theta^* \|^2 \end{equation}","e713652eec7e3e41c4fa4bcc0abbd3bd52bdc459",14752
"4:5380d574-a61c-450b-9d80-55fd2be49de1:42","Equation",,"2025-11-30T08:15:30.817000000Z",,"tex_extractor",,14342,"block","\begin{equation} \label{eq:convergence_rate} \| \theta_t - \theta^* \|^2 \le \left( 1 - \frac{\mu}{L} \right)^t \| \theta_0 - \theta^* \|^2 \end{equation}","6b72669b2596f3ab31b98b75b9a642f66620ef96",15156
"4:5380d574-a61c-450b-9d80-55fd2be49de1:50","Equation",,"2025-11-30T08:15:32.683000000Z",,"tex_extractor",,13211,"inline_equation","\eta \le 1/L","2f899194b0b0515513500195876ed8f9a95e65d1",14025
"4:5380d574-a61c-450b-9d80-55fd2be49de1:51","Equation",,"2025-11-30T08:15:32.794000000Z",,"tex_extractor",,13814,"inline_equation","\eta = \frac{1}{L}","f3601560cead6e76e028a3c6a8c7d70be6a9f4d9",14628
"4:5380d574-a61c-450b-9d80-55fd2be49de1:52","Equation",,"2025-11-30T08:15:32.881000000Z",,"tex_extractor",,14102,"inline_equation","\kappa = L/\mu","70881c54403bc5198da1dd721eb0f89ed3ff4e4e",14916
"4:5380d574-a61c-450b-9d80-55fd2be49de1:53","Equation",,"2025-11-30T08:15:32.968000000Z",,"tex_extractor",,14168,"inline_equation","(1 - \mu/L)","d6cb54c89b72011ef3940e2f5ec0dc2a7390e360",14982
"4:5380d574-a61c-450b-9d80-55fd2be49de1:54","Equation",,"2025-11-30T08:15:33.103000000Z",,"tex_extractor",,15296,"inline_equation","O(1/\sqrt{t})","811d905e2c67362f3a156aa75949a9b7bebf9633",16110